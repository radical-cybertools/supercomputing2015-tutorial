{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Introduction\n",
    "\n",
    "This example shows how the Pilot-Abstraction is used to spawn a Spark job inside of YARN. We show how to combine the Pilot and Spark programming modelling using several examples.\n",
    "\n",
    "\n",
    "   - Spark Documentation: <http://spark.apache.org/docs/latest/>\n",
    "   - Pilot-Spark: <https://pypi.python.org/pypi/SAGA-Hadoop/>\n",
    "\n",
    "\n",
    "## 1. Initialize Spark\n",
    "\n",
    "**Please edit env.py in HOME directory to make sure to point to right Spark installation!**\n",
    "\n",
    "The following codes shows how the Pilot-Abstraction is used to connect to an existing YARN cluster and startup Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK Home: /usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6\n",
      "SPARK HOME: /usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6\n",
      "PYTHONPATH: /usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6/python:/usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\n"
     ]
    }
   ],
   "source": [
    "%run ../env.py\n",
    "%run ../util/init_spark.py\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"yarn-client://yarn-aws.radical-cybertools.org\",\n",
    "    \"number_of_processes\": 2\n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Spark application has been submitted it can be monitored via the YARN web interface: <http://yarn-aws.radical-cybertools.org:8088/> or by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Name</th>\n",
       "      <th>Spark Application URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>radical</td>\n",
       "      <td>Pilot-Spark</td>\n",
       "      <td><a target=blank href='http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0008'>http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0008</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test037</td>\n",
       "      <td>Pilot-Spark</td>\n",
       "      <td><a target=blank href='http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0415'>http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0415</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=!yarn application -list -appTypes Spark -appStates RUNNING\n",
    "print_application_url(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark: Hello RDD Abstraction\n",
    "\n",
    "**Line Count:** How many lines of logs do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891715"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = sc.textFile(\"/data/nasa/\")\n",
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Count:** How many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 2817),\n",
       " (u'[13/Jul/1995:17:48:56', 1),\n",
       " (u'[13/Jul/1995:17:48:54', 1),\n",
       " (u'[13/Jul/1995:17:48:52', 3),\n",
       " (u'/cgi-bin/imagemap/countdown?107,174', 19),\n",
       " (u'[13/Jul/1995:17:48:50', 3),\n",
       " (u'[22/Jul/1995:12:10:41', 1),\n",
       " (u'[03/Jul/1995:02:03:47', 2),\n",
       " (u'[16/Jul/1995:14:34:31', 1),\n",
       " (u'[07/Jul/1995:18:18:55', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP Response Code Count:** How many HTTP errors did we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'403', 54),\n",
       " (u'302', 46573),\n",
       " (u'304', 132627),\n",
       " (u'500', 62),\n",
       " (u'501', 14),\n",
       " (u'200', 1701534),\n",
       " (u'404', 10845),\n",
       " (u'400', 5)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NASA\n",
    "text_rdd = sc.textFile(\"/data/nasa/\")\n",
    "text_rdd.filter(lambda x: len(x)>8).map(lambda x: (x.split()[-2],1)).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "text_filtered = text_rdd.filter(lambda x: len(x)>8)\n",
    "logs = text_filtered.top(20)\n",
    "cleaned = text_filtered.map(lambda l: (l.split(\" \")[0], l.split(\" \")[3][1:], l.split(\" \")[6], l.split(\" \")[-2]))\n",
    "rows = cleaned.map(lambda l: Row(referer=l[0], ts=l[1], response_code=l[3]))\n",
    "schemaLog = sqlContext.createDataFrame(rows)\n",
    "schemaLog.registerTempTable(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(referer=u'199.72.81.55', response_code=u'200', ts=u'01/Jul/1995:00:00:01'),\n",
       " Row(referer=u'unicomp6.unicomp.net', response_code=u'200', ts=u'01/Jul/1995:00:00:06'),\n",
       " Row(referer=u'199.120.110.21', response_code=u'200', ts=u'01/Jul/1995:00:00:09'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'304', ts=u'01/Jul/1995:00:00:11'),\n",
       " Row(referer=u'199.120.110.21', response_code=u'200', ts=u'01/Jul/1995:00:00:11'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'304', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'200', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'205.212.115.106', response_code=u'200', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'd104.aa.net', response_code=u'200', ts=u'01/Jul/1995:00:00:13'),\n",
       " Row(referer=u'129.94.144.152', response_code=u'200', ts=u'01/Jul/1995:00:00:13')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(response_code=u'500', _c1=62),\n",
       " Row(response_code=u'501', _c1=14),\n",
       " Row(response_code=u'400', _c1=5),\n",
       " Row(response_code=u'403', _c1=54),\n",
       " Row(response_code=u'404', _c1=10845),\n",
       " Row(response_code=u'302', _c1=46573),\n",
       " Row(response_code=u'304', _c1=132627),\n",
       " Row(response_code=u'200', _c1=1701534)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select response_code, count(*) from row group by response_code\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Pilot-Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pilot_spark.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Installation Notes\n",
    "\n",
    "Pilot-Spark is part of [SAGA-Hadoop](https://pypi.python.org/pypi/SAGA-Hadoop/)\n",
    "\n",
    "    pip install saga-hadoop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
