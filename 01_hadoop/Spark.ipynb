{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Introduction\n",
    "\n",
    "Spark Documentation: <http://spark.apache.org/docs/latest/>\n",
    "\n",
    "## 1. Initialize Spark\n",
    "\n",
    "**Please edit env.py in HOME directory to make sure to point to right Spark installation!**\n",
    "\n",
    "The following codes shows how the Pilot-Abstraction is used to connect to an existing YARN cluster and startup Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK HOME: /usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6\n",
      "PYTHONPATH: /usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6/python:/usr/hdp/2.3.2.0-2950/spark-1.5.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\n"
     ]
    }
   ],
   "source": [
    "%run ../env.py\n",
    "%run ../util/init_spark.py\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"yarn-client://yarn-aws.radical-cybertools.org\",\n",
    "    \"number_of_processes\": 2\n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Spark application has been submitted it can be monitored via the YARN web interface: <http://yarn-aws.radical-cybertools.org:8088/> or by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:40:16 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:40:16 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "Total number of applications (application-types: [SPARK] and states: [RUNNING]):2\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n",
      "application_1446998777703_0001\t        PySparkShell\t               SPARK\t   radical\t   default\t           RUNNING\t         UNDEFINED\t            10%\t          http://10.99.194.113:4041\n",
      "application_1447000128355_0002\t         Pilot-Spark\t               SPARK\t   radical\t   default\t           RUNNING\t         UNDEFINED\t            10%\t          http://10.99.194.113:4040\n"
     ]
    }
   ],
   "source": [
    "!yarn application -list -appTypes Spark -appStates RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Name</th>\n",
       "      <th>Spark Application URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>radical</td>\n",
       "      <td>Pilot-Spark</td>\n",
       "      <td><a target=blank href='http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0002'>http://yarn-aws.radical-cybertools.org:8088/proxy/application_1447000128355_0002</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=!yarn application -list -appTypes Spark -appStates RUNNING\n",
    "print_application_url(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark: Hello RDD Abstraction\n",
    "\n",
    "Line Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891715"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = sc.textFile(\"/data/nasa/\")\n",
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP Response Code Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'403', 54),\n",
       " (u'302', 46573),\n",
       " (u'304', 132627),\n",
       " (u'500', 62),\n",
       " (u'501', 14),\n",
       " (u'200', 1701534),\n",
       " (u'404', 10845),\n",
       " (u'400', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NASA\n",
    "text_rdd = sc.textFile(\"/data/nasa/\")\n",
    "text_rdd.filter(lambda x: len(x)>8).map(lambda x: (x.split()[-2],1)).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "text_filtered = text_rdd.filter(lambda x: len(x)>8)\n",
    "logs = text_filtered.top(20)\n",
    "cleaned = text_filtered.map(lambda l: (l.split(\" \")[0], l.split(\" \")[3][1:], l.split(\" \")[6], l.split(\" \")[-2]))\n",
    "rows = cleaned.map(lambda l: Row(referer=l[0], ts=l[1], response_code=l[3]))\n",
    "schemaLog = sqlContext.createDataFrame(rows)\n",
    "schemaLog.registerTempTable(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(referer=u'199.72.81.55', response_code=u'200', ts=u'01/Jul/1995:00:00:01'),\n",
       " Row(referer=u'unicomp6.unicomp.net', response_code=u'200', ts=u'01/Jul/1995:00:00:06'),\n",
       " Row(referer=u'199.120.110.21', response_code=u'200', ts=u'01/Jul/1995:00:00:09'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'304', ts=u'01/Jul/1995:00:00:11'),\n",
       " Row(referer=u'199.120.110.21', response_code=u'200', ts=u'01/Jul/1995:00:00:11'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'304', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'burger.letters.com', response_code=u'200', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'205.212.115.106', response_code=u'200', ts=u'01/Jul/1995:00:00:12'),\n",
       " Row(referer=u'd104.aa.net', response_code=u'200', ts=u'01/Jul/1995:00:00:13'),\n",
       " Row(referer=u'129.94.144.152', response_code=u'200', ts=u'01/Jul/1995:00:00:13')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select response_code, count(*) from row group by response_code\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
