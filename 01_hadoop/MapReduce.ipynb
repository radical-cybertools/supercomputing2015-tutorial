{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Hadoop\n",
    "\n",
    "The aim of this exercise is to become familiar with Hadoop. We will show how to run Hadoop application and create our own Python-based streaming application for parsing log file data.\n",
    "\n",
    "## Configure Hadoop Environment and Test Applications\n",
    "\n",
    "For the following exercise we will use two examples provided as part of the standard Hadoop distribution. We use the Hortonworks HDP 2.3.2 deployed on Amazon Web Services (EC2). First we need to set these two variables to the `jar` files containing the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HADOOP_EXAMPLES=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-mapreduce-examples.jar\"\n",
    "HADOOP_STREAMING=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop Services (HDFS and YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Configured Capacity: 425150103552 (395.95 GB)\n",
      "Present Capacity: 390765297664 (363.93 GB)\n",
      "DFS Remaining: 386196246528 (359.67 GB)\n",
      "DFS Used: 4569051136 (4.26 GB)\n",
      "DFS Used%: 1.17%\n",
      "Under replicated blocks: 4\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "report: Access denied for user radical. Superuser privilege is required\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/09 02:24:00 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/09 02:24:00 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "ip-10-63-179-69.ec2.internal:45454\t        RUNNING\tip-10-63-179-69.ec2.internal:8042\t                           0\n",
      "ip-10-145-0-4.ec2.internal:45454\t        RUNNING\tip-10-145-0-4.ec2.internal:8042\t                           1\n",
      "ip-10-218-164-206.ec2.internal:45454\t        RUNNING\tip-10-218-164-206.ec2.internal:8042\t                           1\n",
      "ip-10-179-174-236.ec2.internal:45454\t        RUNNING\tip-10-179-174-236.ec2.internal:8042\t                           1\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terasort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:44:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/teragen' to trash at: hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:45:00 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:45:00 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:45:01 INFO terasort.TeraSort: Generating 100000 using 2\n",
      "15/11/08 16:45:01 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:45:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0005\n",
      "15/11/08 16:45:01 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0005\n",
      "15/11/08 16:45:01 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0005/\n",
      "15/11/08 16:45:01 INFO mapreduce.Job: Running job: job_1447000128355_0005\n",
      "15/11/08 16:45:07 INFO mapreduce.Job: Job job_1447000128355_0005 running in uber mode : false\n",
      "15/11/08 16:45:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:45:14 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "15/11/08 16:45:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:45:16 INFO mapreduce.Job: Job job_1447000128355_0005 completed successfully\n",
      "15/11/08 16:45:16 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=252394\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=164\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8781\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=8781\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8781\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22479360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tInput split bytes=164\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=53\n",
      "\t\tCPU time spent (ms)=2740\n",
      "\t\tPhysical memory (bytes) snapshot=411209728\n",
      "\t\tVirtual memory (bytes) snapshot=5835378688\n",
      "\t\tTotal committed heap usage (bytes)=1225785344\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=214574985129000\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES teragen 100000 teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:43:43 INFO terasort.TeraSort: starting\n",
      "15/11/08 16:43:45 INFO input.FileInputFormat: Total input paths to process : 2\n",
      "Spent 137ms computing base-splits.\n",
      "Spent 2ms computing TeraScheduler splits.\n",
      "Computing input splits took 140ms\n",
      "Sampling 2 splits of 2\n",
      "Making 1 from 100000 sampled records\n",
      "Computing parititions took 394ms\n",
      "Spent 537ms computing partitions.\n",
      "15/11/08 16:43:45 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:43:46 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:43:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:43:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0003\n",
      "15/11/08 16:43:47 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0003\n",
      "15/11/08 16:43:47 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0003/\n",
      "15/11/08 16:43:47 INFO mapreduce.Job: Running job: job_1447000128355_0003\n",
      "15/11/08 16:43:53 INFO mapreduce.Job: Job job_1447000128355_0003 running in uber mode : false\n",
      "15/11/08 16:43:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:44:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:44:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/08 16:44:07 INFO mapreduce.Job: Job job_1447000128355_0003 completed successfully\n",
      "15/11/08 16:44:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10400006\n",
      "\t\tFILE: Number of bytes written=21182781\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10000278\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11663\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8124\n",
      "\t\tTotal time spent by all map tasks (ms)=11663\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4062\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11663\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4062\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29857280\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=20797440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tMap output bytes=10200000\n",
      "\t\tMap output materialized bytes=10400012\n",
      "\t\tInput split bytes=278\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100000\n",
      "\t\tReduce shuffle bytes=10400012\n",
      "\t\tReduce input records=100000\n",
      "\t\tReduce output records=100000\n",
      "\t\tSpilled Records=200000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=116\n",
      "\t\tCPU time spent (ms)=8650\n",
      "\t\tPhysical memory (bytes) snapshot=3613069312\n",
      "\t\tVirtual memory (bytes) snapshot=11025321984\n",
      "\t\tTotal committed heap usage (bytes)=4702863360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10000000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n",
      "15/11/08 16:44:07 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES terasort teragen teraout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Count\n",
    "\n",
    "Count the words contained in the log file located at `/data/nasa/NASA_access_log_Jul95`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `wordcount-out': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\r\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\r\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\r\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\r\n",
      "text: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text /data/nasa/NASA_access_log_Jul95 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:44:14 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:44:15 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:44:15 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "15/11/08 16:44:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:44:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0004\n",
      "15/11/08 16:44:16 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0004\n",
      "15/11/08 16:44:16 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0004/\n",
      "15/11/08 16:44:16 INFO mapreduce.Job: Running job: job_1447000128355_0004\n",
      "15/11/08 16:44:22 INFO mapreduce.Job: Job job_1447000128355_0004 running in uber mode : false\n",
      "15/11/08 16:44:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:44:34 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "15/11/08 16:44:37 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/11/08 16:44:39 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "15/11/08 16:44:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:44:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/08 16:44:51 INFO mapreduce.Job: Job job_1447000128355_0004 completed successfully\n",
      "15/11/08 16:44:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=34517909\n",
      "\t\tFILE: Number of bytes written=69415467\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205373714\n",
      "\t\tHDFS: Number of bytes written=29131159\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39190\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17846\n",
      "\t\tTotal time spent by all map tasks (ms)=39190\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8923\n",
      "\t\tTotal vcore-seconds taken by all map tasks=39190\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8923\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=100326400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=45685760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=18915314\n",
      "\t\tMap output bytes=280900888\n",
      "\t\tMap output materialized bytes=34517915\n",
      "\t\tInput split bytes=274\n",
      "\t\tCombine input records=18915314\n",
      "\t\tCombine output records=1237021\n",
      "\t\tReduce input groups=1216956\n",
      "\t\tReduce shuffle bytes=34517915\n",
      "\t\tReduce input records=1237021\n",
      "\t\tReduce output records=1216956\n",
      "\t\tSpilled Records=2474042\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=942\n",
      "\t\tCPU time spent (ms)=40790\n",
      "\t\tPhysical memory (bytes) snapshot=4047740928\n",
      "\t\tVirtual memory (bytes) snapshot=11030716416\n",
      "\t\tTotal committed heap usage (bytes)=5058330624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205373440\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29131159\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES wordcount /data/nasa/ wordcount-out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log Parsing\n",
    "\n",
    "Use the commands `head`, `cat`, `uniq`, `wc`, `sort`, `find`, `xargs`, `awk` to evaluate the NASA log file:\n",
    "\n",
    "Which page was called the most?\n",
    "What was the most frequent return code?\n",
    "How many errors occurred? What is the percentage of errors?\n",
    "Implement a Python version of this Unix Shell script using this script as template!\n",
    "\n",
    "Run the Python script inside an Hadoop Streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "#\r\n",
      "# Licensed to Cloudera, Inc. under one\r\n",
      "# or more contributor license agreements.  See the NOTICE file\r\n",
      "# distributed with this work for additional information\r\n",
      "# regarding copyright ownership.  Cloudera, Inc. licenses this file\r\n",
      "# to you under the Apache License, Version 2.0 (the\r\n",
      "# \"License\"); you may not use this file except in compliance\r\n",
      "# with the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "#\r\n",
      "# Template for python Hadoop streaming.  Fill in the map() and reduce()\r\n",
      "# functions, which should call emit(), as appropriate.\r\n",
      "#\r\n",
      "# Test your script with\r\n",
      "#  cat input | python map_reduce.py map | sort | python wordcount.py reduce\r\n",
      "\r\n",
      "import sys\r\n",
      "import re\r\n",
      "try:\r\n",
      "    import simplejson as json\r\n",
      "except ImportError:\r\n",
      "    import json\r\n",
      "\r\n",
      "import __builtin__\r\n",
      "\r\n",
      "def map(line):\r\n",
      "    try:\r\n",
      "        words = line.split()\r\n",
      "        http_response_code = words[-2]\r\n",
      "        emit(http_response_code, str(1))\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "    \r\n",
      "def reduce(key, values):\r\n",
      "    emit(key, str(sum(__builtin__.map(int,values))))\r\n",
      "\r\n",
      "# Common library code follows:\r\n",
      "\r\n",
      "def emit(key, value):\r\n",
      "    \"\"\"\r\n",
      "    Emits a key->value pair.  Key and value should be strings.\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        print \"\\t\".join( (key, value) )\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "\r\n",
      "def run_map():\r\n",
      "    \"\"\"Calls map() for each input value.\"\"\"\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        map(line)\r\n",
      "\r\n",
      "def run_reduce():\r\n",
      "    \"\"\"Gathers reduce() data in memory, and calls reduce().\"\"\"\r\n",
      "    prev_key = None\r\n",
      "    values = []\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        key, value = re.split(\"\\t\", line, 1)\r\n",
      "        if prev_key == key:\r\n",
      "            values.append(value)\r\n",
      "        else:\r\n",
      "            if prev_key is not None:\r\n",
      "                reduce(prev_key, values)\r\n",
      "            prev_key = key\r\n",
      "            values = [ value ]\r\n",
      "\r\n",
      "    if prev_key is not None:\r\n",
      "        reduce(prev_key, values)\r\n",
      "\r\n",
      "def main():\r\n",
      "    \"\"\"Runs map or reduce code, per arguments.\"\"\"\r\n",
      "    if len(sys.argv) != 2 or sys.argv[1] not in (\"map\", \"reduce\"):\r\n",
      "        print \"Usage: %s <map|reduce>\" % sys.argv[0]\r\n",
      "        sys.exit(1)\r\n",
      "    if sys.argv[1] == \"map\":\r\n",
      "        run_map()\r\n",
      "    elif sys.argv[1] == \"reduce\":\r\n",
      "        run_reduce()\r\n",
      "    else:\r\n",
      "        assert False\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "  main()\r\n"
     ]
    }
   ],
   "source": [
    "!cat mapreduce_streaming.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:48:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapreduce_streaming.py] [/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming-2.7.1.2.3.2.0-2950.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob6403632065434180035.jar tmpDir=null\n",
      "15/11/08 16:48:30 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:48:30 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:48:30 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:48:30 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:48:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/11/08 16:48:31 INFO net.NetworkTopology: Adding a new node: /default-rack/10.179.174.236:50010\n",
      "15/11/08 16:48:31 INFO net.NetworkTopology: Adding a new node: /default-rack/10.218.164.206:50010\n",
      "15/11/08 16:48:31 INFO net.NetworkTopology: Adding a new node: /default-rack/10.145.0.4:50010\n",
      "15/11/08 16:48:31 INFO net.NetworkTopology: Adding a new node: /default-rack/10.63.179.69:50010\n",
      "15/11/08 16:48:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:48:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0006\n",
      "15/11/08 16:48:31 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0006\n",
      "15/11/08 16:48:31 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0006/\n",
      "15/11/08 16:48:31 INFO mapreduce.Job: Running job: job_1447000128355_0006\n",
      "15/11/08 16:48:38 INFO mapreduce.Job: Job job_1447000128355_0006 running in uber mode : false\n",
      "15/11/08 16:48:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:48:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:48:57 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "15/11/08 16:48:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/08 16:49:01 INFO mapreduce.Job: Job job_1447000128355_0006 completed successfully\n",
      "15/11/08 16:49:01 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15133718\n",
      "\t\tFILE: Number of bytes written=30656111\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205250808\n",
      "\t\tHDFS: Number of bytes written=70\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13759\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21326\n",
      "\t\tTotal time spent by all map tasks (ms)=13759\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10663\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13759\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10663\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=35223040\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=54594560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=1891714\n",
      "\t\tMap output bytes=11350284\n",
      "\t\tMap output materialized bytes=15133724\n",
      "\t\tInput split bytes=248\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=15133724\n",
      "\t\tReduce input records=1891714\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=3783428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=157\n",
      "\t\tCPU time spent (ms)=16960\n",
      "\t\tPhysical memory (bytes) snapshot=3724726272\n",
      "\t\tVirtual memory (bytes) snapshot=11014062080\n",
      "\t\tTotal committed heap usage (bytes)=4794613760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205250560\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70\n",
      "15/11/08 16:49:01 INFO streaming.StreamJob: Output directory: logs-parsed\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_STREAMING -input /data/nasa -output logs-parsed \\\n",
    "                            -file mapreduce_streaming.py \\\n",
    "                            -mapper \"python mapreduce_streaming.py map\" \\\n",
    "                            -reducer \"python mapreduce_streaming.py reduce\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 radical hdfs          0 2015-11-08 16:48 logs-parsed/_SUCCESS\r\n",
      "-rw-r--r--   3 radical hdfs         70 2015-11-08 16:48 logs-parsed/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls logs-parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\t1701534\r\n",
      "302\t46573\r\n",
      "304\t132627\r\n",
      "400\t5\r\n",
      "403\t54\r\n",
      "404\t10845\r\n",
      "500\t62\r\n",
      "501\t14\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text logs-parsed/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
