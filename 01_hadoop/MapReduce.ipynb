{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Hadoop\n",
    "\n",
    "The aim of this exercise is to become familiar with Hadoop. We will show how to run Hadoop application and create our own Python-based streaming application for parsing log file data.\n",
    "\n",
    "## Configure Hadoop Environment and Test Applications\n",
    "\n",
    "For the following exercise we will use two examples provided as part of the standard Hadoop distribution. We use the Hortonworks HDP 2.3.2 deployed on Amazon Web Services (EC2). First we need to set these two variables to the `jar` files containing the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HADOOP_EXAMPLES=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-mapreduce-examples.jar\"\n",
    "HADOOP_STREAMING=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the standard Hadoop command line utilities to enquire the status of the cluster (in particular HDFS and Yarn), and to submit applications. Shell commands can be executed in iPython notebook cells using the prefix `!`. Use shift-enter or the play button in the menu to execute cells. \n",
    "\n",
    "## 1. Hadoop Services (HDFS and YARN)\n",
    "\n",
    "The `-report` argument provides information about the file-system, disk space, number of nodes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Configured Capacity: 638810435584 (594.94 GB)\n",
      "Present Capacity: 572727713792 (533.39 GB)\n",
      "DFS Remaining: 504732938240 (470.07 GB)\n",
      "DFS Used: 67994775552 (63.33 GB)\n",
      "DFS Used%: 11.87%\n",
      "Under replicated blocks: 4\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "report: Access denied for user radical. Superuser privilege is required\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:38:14 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 15:38:14 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "Total Nodes:6\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "ip-10-63-179-69.ec2.internal:45454\t        RUNNING\tip-10-63-179-69.ec2.internal:8042\t                           0\n",
      "ip-10-47-179-69.ec2.internal:45454\t        RUNNING\tip-10-47-179-69.ec2.internal:8042\t                           0\n",
      "ip-10-145-0-4.ec2.internal:45454\t        RUNNING\tip-10-145-0-4.ec2.internal:8042\t                           0\n",
      "ip-10-99-194-113.ec2.internal:45454\t        RUNNING\tip-10-99-194-113.ec2.internal:8042\t                           0\n",
      "ip-10-218-164-206.ec2.internal:45454\t        RUNNING\tip-10-218-164-206.ec2.internal:8042\t                           0\n",
      "ip-10-179-174-236.ec2.internal:45454\t        RUNNING\tip-10-179-174-236.ec2.internal:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terasort\n",
    "\n",
    "The Terasort application consists of two parts: (i) data generation (`teragen`), and (ii) the sorting of the data (`terasort`). \n",
    "\n",
    "Note that if you execute `teragen` multiple times, please make sure to delete the target directory (`teragen`); otherwise an error is thrown as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:38:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\r\n",
      "Moved: 'hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/teragen' to trash at: hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/.Trash/Current\r\n",
      "15/11/15 15:38:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\r\n",
      "Moved: 'hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/teraout' to trash at: hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/.Trash/Current\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r teragen teraout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next command we will create a dataset of 100,000 records,  each of 100 bytes (for a total of 10Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:39:29 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 15:39:29 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/15 15:39:30 INFO terasort.TeraSort: Generating 100000 using 2\n",
      "15/11/15 15:39:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/15 15:39:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0778\n",
      "15/11/15 15:39:31 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0778\n",
      "15/11/15 15:39:31 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0778/\n",
      "15/11/15 15:39:31 INFO mapreduce.Job: Running job: job_1447000128355_0778\n",
      "15/11/15 15:39:37 INFO mapreduce.Job: Job job_1447000128355_0778 running in uber mode : false\n",
      "15/11/15 15:39:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/15 15:39:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/15 15:39:43 INFO mapreduce.Job: Job job_1447000128355_0778 completed successfully\n",
      "15/11/15 15:39:43 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=252368\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=164\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6485\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=6485\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6485\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=16601600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tInput split bytes=164\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=50\n",
      "\t\tCPU time spent (ms)=2540\n",
      "\t\tPhysical memory (bytes) snapshot=374366208\n",
      "\t\tVirtual memory (bytes) snapshot=5856428032\n",
      "\t\tTotal committed heap usage (bytes)=1516765184\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=214574985129000\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES teragen 100000 teragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following command will use Hadoop commands (`yarn`) to perform sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:45:22 INFO terasort.TeraSort: starting\n",
      "15/11/15 15:45:23 INFO input.FileInputFormat: Total input paths to process : 2\n",
      "Spent 126ms computing base-splits.\n",
      "Spent 2ms computing TeraScheduler splits.\n",
      "Computing input splits took 129ms\n",
      "Sampling 2 splits of 2\n",
      "Making 1 from 100000 sampled records\n",
      "Computing parititions took 358ms\n",
      "Spent 490ms computing partitions.\n",
      "15/11/15 15:45:24 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 15:45:24 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/teraout already exists\n",
      "\tat org.apache.hadoop.examples.terasort.TeraOutputFormat.checkOutputSpecs(TeraOutputFormat.java:114)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n",
      "\tat org.apache.hadoop.examples.terasort.TeraSort.run(TeraSort.java:316)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.hadoop.examples.terasort.TeraSort.main(TeraSort.java:325)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
      "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
      "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES terasort teragen teraout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [   ]
    }
   ],
   "source": [
    "!hadoop fs -text teraout/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Word Count\n",
    "\n",
    "Here we will discuss the `hello world` of Big Data: word count! We will count the words contained in the log file located at `/data/nasa/NASA_access_log_Jul95`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:46:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/wordcount-out' to trash at: hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\r\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\r\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\r\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\r\n",
      "text: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text /data/nasa/NASA_access_log_Jul95 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 15:47:07 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 15:47:07 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/15 15:47:08 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "15/11/15 15:47:08 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/15 15:47:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0807\n",
      "15/11/15 15:47:09 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0807\n",
      "15/11/15 15:47:09 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0807/\n",
      "15/11/15 15:47:09 INFO mapreduce.Job: Running job: job_1447000128355_0807\n",
      "15/11/15 15:47:15 INFO mapreduce.Job: Job job_1447000128355_0807 running in uber mode : false\n",
      "15/11/15 15:47:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/15 15:47:27 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "15/11/15 15:47:31 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "15/11/15 15:47:34 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "15/11/15 15:47:37 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "15/11/15 15:47:40 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/11/15 15:47:47 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "15/11/15 15:47:58 INFO mapreduce.Job:  map 83% reduce 17%\n",
      "15/11/15 15:48:02 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "15/11/15 15:48:04 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "15/11/15 15:48:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/15 15:48:07 INFO mapreduce.Job: Job job_1447000128355_0807 completed successfully\n",
      "15/11/15 15:48:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=34517909\n",
      "\t\tFILE: Number of bytes written=69415428\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205373714\n",
      "\t\tHDFS: Number of bytes written=29131159\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=74953\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31964\n",
      "\t\tTotal time spent by all map tasks (ms)=74953\n",
      "\t\tTotal time spent by all reduce tasks (ms)=15982\n",
      "\t\tTotal vcore-seconds taken by all map tasks=74953\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=15982\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=191879680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=81827840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=18915314\n",
      "\t\tMap output bytes=280900888\n",
      "\t\tMap output materialized bytes=34517915\n",
      "\t\tInput split bytes=274\n",
      "\t\tCombine input records=18915314\n",
      "\t\tCombine output records=1237021\n",
      "\t\tReduce input groups=1216956\n",
      "\t\tReduce shuffle bytes=34517915\n",
      "\t\tReduce input records=1237021\n",
      "\t\tReduce output records=1216956\n",
      "\t\tSpilled Records=2474042\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=2380\n",
      "\t\tCPU time spent (ms)=56880\n",
      "\t\tPhysical memory (bytes) snapshot=4037234688\n",
      "\t\tVirtual memory (bytes) snapshot=11021074432\n",
      "\t\tTotal committed heap usage (bytes)=4972347392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205373440\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29131159\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES wordcount /data/nasa/ wordcount-out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log Parsing\n",
    "\n",
    "Use the commands `head`, `cat`, `uniq`, `wc`, `sort`, `find`, `xargs`, `awk` to evaluate the NASA log file:\n",
    "\n",
    "Which page was called the most?\n",
    "What was the most frequent return code?\n",
    "How many errors occurred? What is the percentage of errors? Implement a Python version of this Unix Shell script using the script as a template (the answer can be found  in [mapreduce_streaming.py](mapreduce_streaming.py))\n",
    "\n",
    "We will now run the Python script inside a Hadoop Streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "#\r\n",
      "# Licensed to Cloudera, Inc. under one\r\n",
      "# or more contributor license agreements.  See the NOTICE file\r\n",
      "# distributed with this work for additional information\r\n",
      "# regarding copyright ownership.  Cloudera, Inc. licenses this file\r\n",
      "# to you under the Apache License, Version 2.0 (the\r\n",
      "# \"License\"); you may not use this file except in compliance\r\n",
      "# with the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "#\r\n",
      "# Template for python Hadoop streaming.  Fill in the map() and reduce()\r\n",
      "# functions, which should call emit(), as appropriate.\r\n",
      "#\r\n",
      "# Test your script with\r\n",
      "#  cat input | python map_reduce.py map | sort | python wordcount.py reduce\r\n",
      "\r\n",
      "import sys\r\n",
      "import re\r\n",
      "try:\r\n",
      "    import simplejson as json\r\n",
      "except ImportError:\r\n",
      "    import json\r\n",
      "\r\n",
      "import __builtin__\r\n",
      "\r\n",
      "def map(line):\r\n",
      "    try:\r\n",
      "        words = line.split()\r\n",
      "        http_response_code = words[-2]\r\n",
      "        emit(http_response_code, str(1))\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "    \r\n",
      "def reduce(key, values):\r\n",
      "    emit(key, str(sum(__builtin__.map(int,values))))\r\n",
      "\r\n",
      "# Common library code follows:\r\n",
      "\r\n",
      "def emit(key, value):\r\n",
      "    \"\"\"\r\n",
      "    Emits a key->value pair.  Key and value should be strings.\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        print \"\\t\".join( (key, value) )\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "\r\n",
      "def run_map():\r\n",
      "    \"\"\"Calls map() for each input value.\"\"\"\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        map(line)\r\n",
      "\r\n",
      "def run_reduce():\r\n",
      "    \"\"\"Gathers reduce() data in memory, and calls reduce().\"\"\"\r\n",
      "    prev_key = None\r\n",
      "    values = []\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        key, value = re.split(\"\\t\", line, 1)\r\n",
      "        if prev_key == key:\r\n",
      "            values.append(value)\r\n",
      "        else:\r\n",
      "            if prev_key is not None:\r\n",
      "                reduce(prev_key, values)\r\n",
      "            prev_key = key\r\n",
      "            values = [ value ]\r\n",
      "\r\n",
      "    if prev_key is not None:\r\n",
      "        reduce(prev_key, values)\r\n",
      "\r\n",
      "def main():\r\n",
      "    \"\"\"Runs map or reduce code, per arguments.\"\"\"\r\n",
      "    if len(sys.argv) != 2 or sys.argv[1] not in (\"map\", \"reduce\"):\r\n",
      "        print \"Usage: %s <map|reduce>\" % sys.argv[0]\r\n",
      "        sys.exit(1)\r\n",
      "    if sys.argv[1] == \"map\":\r\n",
      "        run_map()\r\n",
      "    elif sys.argv[1] == \"reduce\":\r\n",
      "        run_reduce()\r\n",
      "    else:\r\n",
      "        assert False\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "  main()\r\n"
     ]
    }
   ],
   "source": [
    "!cat mapreduce_streaming.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the next example, we'll execute `mapreduce_streaming.py` with YARN as a hadoop streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/15 03:35:20 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapreduce_streaming.py] [/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming-2.7.1.2.3.2.0-2950.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob2404468226140792590.jar tmpDir=null\n",
      "15/11/15 03:35:21 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 03:35:21 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/15 03:35:22 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/15 03:35:22 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/15 03:35:22 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/logs-parsed already exists\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_STREAMING -input /data/nasa -output logs-parsed \\\n",
    "                            -file mapreduce_streaming.py \\\n",
    "                            -mapper \"python mapreduce_streaming.py map\" \\\n",
    "                            -reducer \"python mapreduce_streaming.py reduce\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the next two commands, we'll parse the output of the operations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 radical hdfs          0 2015-11-08 16:48 logs-parsed/_SUCCESS\r\n",
      "-rw-r--r--   3 radical hdfs         70 2015-11-08 16:48 logs-parsed/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls logs-parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\t1701534\r\n",
      "302\t46573\r\n",
      "304\t132627\r\n",
      "400\t5\r\n",
      "403\t54\r\n",
      "404\t10845\r\n",
      "500\t62\r\n",
      "501\t14\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text logs-parsed/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
