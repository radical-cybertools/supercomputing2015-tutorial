{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Hadoop\n",
    "\n",
    "The aim of this exercise is to get familiar with Hadoop. We will show how to run Hadoop application and create our own Python-based streaming application for parsing log file data.\n",
    "\n",
    "## Configure Hadoop Environment and Test Applications\n",
    "\n",
    "For the following exercise we will use two examples provided as part of the standard Hadoop distribution. We use the Hortonworks HDP 2.3.2 deployed on Amazon Web Services (EC2). First we need to set these two variables to the `jar` files containing the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HADOOP_EXAMPLES=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-mapreduce-examples.jar\"\n",
    "HADOOP_STREAMING=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop Services (HDFS and YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Configured Capacity: 425150103552 (395.95 GB)\n",
      "Present Capacity: 392212938752 (365.28 GB)\n",
      "DFS Remaining: 389594132480 (362.84 GB)\n",
      "DFS Used: 2618806272 (2.44 GB)\n",
      "DFS Used%: 0.67%\n",
      "Under replicated blocks: 4\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "report: Access denied for user radical. Superuser privilege is required\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:43:39 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:43:39 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "ip-10-63-179-69.ec2.internal:45454\t        RUNNING\tip-10-63-179-69.ec2.internal:8042\t                           0\n",
      "ip-10-145-0-4.ec2.internal:45454\t        RUNNING\tip-10-145-0-4.ec2.internal:8042\t                           1\n",
      "ip-10-218-164-206.ec2.internal:45454\t        RUNNING\tip-10-218-164-206.ec2.internal:8042\t                           1\n",
      "ip-10-179-174-236.ec2.internal:45454\t        RUNNING\tip-10-179-174-236.ec2.internal:8042\t                           1\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terasort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:44:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/teragen' to trash at: hdfs://ip-10-63-179-69.ec2.internal:8020/user/radical/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:45:00 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:45:00 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:45:01 INFO terasort.TeraSort: Generating 100000 using 2\n",
      "15/11/08 16:45:01 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:45:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0005\n",
      "15/11/08 16:45:01 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0005\n",
      "15/11/08 16:45:01 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0005/\n",
      "15/11/08 16:45:01 INFO mapreduce.Job: Running job: job_1447000128355_0005\n",
      "15/11/08 16:45:07 INFO mapreduce.Job: Job job_1447000128355_0005 running in uber mode : false\n",
      "15/11/08 16:45:07 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:45:14 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "15/11/08 16:45:15 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:45:16 INFO mapreduce.Job: Job job_1447000128355_0005 completed successfully\n",
      "15/11/08 16:45:16 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=252394\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=164\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8781\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=8781\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8781\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22479360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tInput split bytes=164\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=53\n",
      "\t\tCPU time spent (ms)=2740\n",
      "\t\tPhysical memory (bytes) snapshot=411209728\n",
      "\t\tVirtual memory (bytes) snapshot=5835378688\n",
      "\t\tTotal committed heap usage (bytes)=1225785344\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=214574985129000\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES teragen 100000 teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:43:43 INFO terasort.TeraSort: starting\n",
      "15/11/08 16:43:45 INFO input.FileInputFormat: Total input paths to process : 2\n",
      "Spent 137ms computing base-splits.\n",
      "Spent 2ms computing TeraScheduler splits.\n",
      "Computing input splits took 140ms\n",
      "Sampling 2 splits of 2\n",
      "Making 1 from 100000 sampled records\n",
      "Computing parititions took 394ms\n",
      "Spent 537ms computing partitions.\n",
      "15/11/08 16:43:45 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:43:46 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:43:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:43:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0003\n",
      "15/11/08 16:43:47 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0003\n",
      "15/11/08 16:43:47 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0003/\n",
      "15/11/08 16:43:47 INFO mapreduce.Job: Running job: job_1447000128355_0003\n",
      "15/11/08 16:43:53 INFO mapreduce.Job: Job job_1447000128355_0003 running in uber mode : false\n",
      "15/11/08 16:43:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:44:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:44:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/08 16:44:07 INFO mapreduce.Job: Job job_1447000128355_0003 completed successfully\n",
      "15/11/08 16:44:07 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10400006\n",
      "\t\tFILE: Number of bytes written=21182781\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10000278\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11663\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8124\n",
      "\t\tTotal time spent by all map tasks (ms)=11663\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4062\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11663\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4062\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=29857280\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=20797440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tMap output bytes=10200000\n",
      "\t\tMap output materialized bytes=10400012\n",
      "\t\tInput split bytes=278\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100000\n",
      "\t\tReduce shuffle bytes=10400012\n",
      "\t\tReduce input records=100000\n",
      "\t\tReduce output records=100000\n",
      "\t\tSpilled Records=200000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=116\n",
      "\t\tCPU time spent (ms)=8650\n",
      "\t\tPhysical memory (bytes) snapshot=3613069312\n",
      "\t\tVirtual memory (bytes) snapshot=11025321984\n",
      "\t\tTotal committed heap usage (bytes)=4702863360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10000000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n",
      "15/11/08 16:44:07 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES terasort teragen teraout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Count\n",
    "\n",
    "Count the words contained in the log file located at `/data/nasa/NASA_access_log_Jul95`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `wordcount-out': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\r\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\r\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\r\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\r\n",
      "text: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text /data/nasa/NASA_access_log_Jul95 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:44:14 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-63-179-69.ec2.internal:8188/ws/v1/timeline/\n",
      "15/11/08 16:44:15 INFO client.RMProxy: Connecting to ResourceManager at ip-10-63-179-69.ec2.internal/10.63.179.69:8050\n",
      "15/11/08 16:44:15 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "15/11/08 16:44:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/11/08 16:44:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1447000128355_0004\n",
      "15/11/08 16:44:16 INFO impl.YarnClientImpl: Submitted application application_1447000128355_0004\n",
      "15/11/08 16:44:16 INFO mapreduce.Job: The url to track the job: http://ip-10-63-179-69.ec2.internal:8088/proxy/application_1447000128355_0004/\n",
      "15/11/08 16:44:16 INFO mapreduce.Job: Running job: job_1447000128355_0004\n",
      "15/11/08 16:44:22 INFO mapreduce.Job: Job job_1447000128355_0004 running in uber mode : false\n",
      "15/11/08 16:44:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/11/08 16:44:34 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "15/11/08 16:44:37 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/11/08 16:44:39 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "15/11/08 16:44:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/11/08 16:44:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/11/08 16:44:51 INFO mapreduce.Job: Job job_1447000128355_0004 completed successfully\n",
      "15/11/08 16:44:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=34517909\n",
      "\t\tFILE: Number of bytes written=69415467\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205373714\n",
      "\t\tHDFS: Number of bytes written=29131159\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39190\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17846\n",
      "\t\tTotal time spent by all map tasks (ms)=39190\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8923\n",
      "\t\tTotal vcore-seconds taken by all map tasks=39190\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8923\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=100326400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=45685760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=18915314\n",
      "\t\tMap output bytes=280900888\n",
      "\t\tMap output materialized bytes=34517915\n",
      "\t\tInput split bytes=274\n",
      "\t\tCombine input records=18915314\n",
      "\t\tCombine output records=1237021\n",
      "\t\tReduce input groups=1216956\n",
      "\t\tReduce shuffle bytes=34517915\n",
      "\t\tReduce input records=1237021\n",
      "\t\tReduce output records=1216956\n",
      "\t\tSpilled Records=2474042\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=942\n",
      "\t\tCPU time spent (ms)=40790\n",
      "\t\tPhysical memory (bytes) snapshot=4047740928\n",
      "\t\tVirtual memory (bytes) snapshot=11030716416\n",
      "\t\tTotal committed heap usage (bytes)=5058330624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205373440\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29131159\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES wordcount /data/nasa/ wordcount-out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log Parsing\n",
    "\n",
    "Use the commands `head`, `cat`, `uniq`, `wc`, `sort`, `find`, `xargs`, `awk` to evaluate the NASA log file:\n",
    "\n",
    "Which page was called the most?\n",
    "What was the most frequent return code?\n",
    "How many errors occurred? What is the percentage of errors?\n",
    "Implement a Python version of this Unix Shell script using this script as template!\n",
    "\n",
    "Run the Python script inside an Hadoop Streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: mapreduce_streaming.py: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat mapreduce_streaming.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/11/08 16:47:11 ERROR streaming.StreamJob: Unrecognized option: -files\r\n",
      "Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\r\n",
      "Options:\r\n",
      "  -input          <path> DFS input file(s) for the Map step.\r\n",
      "  -output         <path> DFS output directory for the Reduce step.\r\n",
      "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\r\n",
      "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\r\n",
      "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\r\n",
      "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\r\n",
      "                  Deprecated. Use generic option \"-files\" instead.\r\n",
      "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\r\n",
      "                  Optional. The input format class.\r\n",
      "  -outputformat   <TextOutputFormat(default)|JavaClassName>\r\n",
      "                  Optional. The output format class.\r\n",
      "  -partitioner    <JavaClassName>  Optional. The partitioner class.\r\n",
      "  -numReduceTasks <num> Optional. Number of reduce tasks.\r\n",
      "  -inputreader    <spec> Optional. Input recordreader spec.\r\n",
      "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\r\n",
      "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\r\n",
      "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\r\n",
      "  -io             <identifier> Optional. Format to use for input to and output\r\n",
      "                  from mapper/reducer commands\r\n",
      "  -lazyOutput     Optional. Lazily create Output.\r\n",
      "  -background     Optional. Submit the job and don't wait till it completes.\r\n",
      "  -verbose        Optional. Print verbose output.\r\n",
      "  -info           Optional. Print detailed usage.\r\n",
      "  -help           Optional. Print help message.\r\n",
      "\r\n",
      "Generic options supported are\r\n",
      "-conf <configuration file>     specify an application configuration file\r\n",
      "-D <property=value>            use value for given property\r\n",
      "-fs <local|namenode:port>      specify a namenode\r\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\r\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\r\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\r\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\r\n",
      "\r\n",
      "The general command line syntax is\r\n",
      "bin/hadoop command [genericOptions] [commandOptions]\r\n",
      "\r\n",
      "\r\n",
      "For more details about these options:\r\n",
      "Use $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\r\n",
      "\r\n",
      "Try -help for more information\r\n",
      "Streaming Command Failed!\r\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_STREAMING -input /data/nasa -output logs-parsed \\\n",
    "                            -files mapreduce_streaming.py \\\n",
    "                            -mapper \"python mapreduce_streaming.py map\" \\\n",
    "                            -reducer \"python mapreduce_streaming.py reduce\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `logs-parsed': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls logs-parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
