{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Intensive Applications on HPC Using Hadoop, Spark and RADICAL-Cybertools\n",
    "\n",
    "*Shantenu Jha and Andre Luckow*\n",
    "\n",
    "The tutorial material is available as iPython notebook on Github:\n",
    "\n",
    "* https://github.com/radical-cybertools/supercomputing2015-tutorial\n",
    "\n",
    "## Requirements and Setup:\n",
    "\n",
    "Python with the following libraries:\n",
    "\n",
    "* iPython\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Scikit-Learn\n",
    "* Matplotlib, Seaborn\n",
    "\n",
    "We recommend to use [Anaconda](http://continuum.io/downloads).\n",
    "\n",
    "\n",
    "## 1. Hadoop and Spark Introduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Pilot-Abstraction for distributed HPC and Apache Hadoop Big Data Stack (ABDS)\n",
    "\n",
    "The Pilot-Abstraction has been successfully used in HPC for supporting a diverse set of task-based workloads on distributed resources. A Pilot-Job is a placeholder job that is submitting to the resource management system and is used as a container for a dynamically determined set of compute tasks. The Pilot-Data abstraction extends the Pilot-Abstraction for supporting the management of data in conjunction with compute tasks. \n",
    "\n",
    "The Pilot-Abstraction supports a heterogeneous resources, in particular different kinds of cloud, HPC and Hadoop resources.\n",
    "\n",
    "![Pilot Abstraction](./figures/interoperable_pilot_job.png)\n",
    "\n",
    "The following example demonstrates how the Pilot-Abstraction is used to manage a set of compute tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pairplots show the scatter-plot between each of the four features. Clusters for the different species are indicated by the color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 KMeans (Spark)\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-clustering.html#k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlCtx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b3a1e3f81364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_spark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlCtx' is not defined"
     ]
    }
   ],
   "source": [
    "data_spark=sqlCtx.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLength SepalWidth PetalLength PetalWidth\n",
      "5.1         3.5        1.4         0.2       \n",
      "4.9         3.0        1.4         0.2       \n",
      "4.7         3.2        1.3         0.2       \n",
      "4.6         3.1        1.5         0.2       \n",
      "5.0         3.6        1.4         0.2       \n",
      "5.4         3.9        1.7         0.4       \n",
      "4.6         3.4        1.4         0.3       \n",
      "5.0         3.4        1.5         0.2       \n",
      "4.4         2.9        1.4         0.2       \n",
      "4.9         3.1        1.5         0.1       \n",
      "5.4         3.7        1.5         0.2       \n",
      "4.8         3.4        1.6         0.2       \n",
      "4.8         3.0        1.4         0.1       \n",
      "4.3         3.0        1.1         0.1       \n",
      "5.8         4.0        1.2         0.2       \n",
      "5.7         4.4        1.5         0.4       \n",
      "5.4         3.9        1.3         0.4       \n",
      "5.1         3.5        1.4         0.3       \n",
      "5.7         3.8        1.7         0.3       \n",
      "5.1         3.8        1.5         0.3       \n"
     ]
    }
   ],
   "source": [
    "data_spark_without_class=data_spark.select('SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame to Tuple for MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_spark_tuple = data_spark.map(lambda a: (a[0],a[1],a[2],a[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLlib KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model (cluster the data)\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "clusters = KMeans.train(data_spark_tuple, 3, maxIterations=10,\n",
    "                        runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 97.3259242343\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = data_spark_tuple.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future Work: Midas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Midas](figures/midas.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
