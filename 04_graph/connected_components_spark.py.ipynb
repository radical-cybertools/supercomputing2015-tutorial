{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Spark: 1.4.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "\n",
    "# SPARK_HOME=\"/home1/01131/tg804093/src/spark-1.4.1-bin-hadoop2.6\"\n",
    "SPARK_HOME=\"/home1/01131/tg804093/src/supercomputing2015-tutorial/02_hadoop_on_hpc/work/spark-1.4.1-bin-hadoop2.6\"\n",
    "os.environ[\"SPARK_HOME\"]=SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python/lib/py4j-0.8.2.1-src.zip')) \n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'bin') )\n",
    "\n",
    "# import Spark Libraries\n",
    "from pyspark import SparkContext, SparkConf, Accumulator, AccumulatorParam\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import Vector\n",
    "\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc = SparkContext(\"local[1]\")\n",
    "    sqlCtx=SQLContext(sc)\n",
    "\n",
    "print \"Loaded Spark: %s\"%(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graph_edges_10000_29735.csv', 'graph_edges_1000000_2983785.csv', 'graph_edges_10000000_29860787.csv', 'graph_edges_10000_29752.csv', 'graph_edges_100000_298441.csv']\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR=\"/work/01131/tg804093/graph\"\n",
    "files = os.listdir(OUTPUT_DIR)\n",
    "print str(files)\n",
    "for i in files:\n",
    "    filename = os.path.join(OUTPUT_DIR, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Time: 0.0 sec\n",
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start = time.time()\n",
    "filename = os.path.join(OUTPUT_DIR, \"graph_edges_10000000_29860787.csv\")\n",
    "data = sc.textFile(filename).map(lambda line: [int(line.split(\",\")[0]), int(line.split(\",\")[1])])\n",
    "# add backward edges\n",
    "data = data.flatMap(lambda v: [(v[0],v[1]),(v[1],v[0])])\n",
    "data_grouped = data.groupByKey().mapValues(lambda a: sorted(set(a))).cache()\n",
    "end = time.time()\n",
    "                                               \n",
    "read_time = end - start\n",
    "print \"Read Time: %.1f sec\"%read_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [15978, 22569, 39097, 73662, 78836])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "new_iteration_needed = sc.accumulator(0)\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    source = vertex[0]\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = vertex[1][0]\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    for current_destination in vertex[1]:\n",
    "        #print \"Current destination: %s\"%str(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    #print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex((19, [7, 9, 19, 41]))\n",
    "num_iterations=0\n",
    "cc = data_grouped\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    #print \"Accum before iteration: \" + str(old_accum_value)\n",
    "    cc = cc.flatMap(lambda v: process_vertex(v))\\\n",
    "           .groupByKey()\\\n",
    "           .mapValues(lambda a: sorted(set(a)))\n",
    "    cc.collect()\n",
    "    num_iterations = num_iterations + 1\n",
    "    #print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, cc.count(), (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
