{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Hadoop\n",
    "\n",
    "## Configure Hadoop Environment and Test Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HADOOP_EXAMPLES=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-mapreduce-examples.jar\"\n",
    "HADOOP_STREAMING=\"/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hadoop Services (HDFS and YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "Configured Capacity: 2591888113664 (2.36 TB)\n",
      "Present Capacity: 2501066907648 (2.27 TB)\n",
      "DFS Remaining: 2445981851648 (2.22 TB)\n",
      "DFS Used: 55085056000 (51.30 GB)\n",
      "DFS Used%: 2.20%\n",
      "Under replicated blocks: 4\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "report: Access denied for user radical. Superuser privilege is required\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/30 23:03:48 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/30 23:03:48 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "Total Nodes:8\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      " radical-5:45454\t        RUNNING\t   radical-5:8042\t                           0\n",
      " radical-8:45454\t        RUNNING\t   radical-8:8042\t                           0\n",
      " radical-7:45454\t        RUNNING\t   radical-7:8042\t                           0\n",
      " radical-3:45454\t        RUNNING\t   radical-3:8042\t                           0\n",
      " radical-4:45454\t        RUNNING\t   radical-4:8042\t                           0\n",
      " radical-6:45454\t        RUNNING\t   radical-6:8042\t                           0\n",
      " radical-9:45454\t        RUNNING\t   radical-9:8042\t                           0\n",
      " radical-2:45454\t        RUNNING\t   radical-2:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terasort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use \"yarn jar\" to launch YARN applications.\n",
      "15/10/30 23:03:51 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/30 23:03:51 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "15/10/30 23:03:52 INFO terasort.TeraSort: Generating 100000 using 2\n",
      "15/10/30 23:03:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/10/30 23:03:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446245362115_0001\n",
      "15/10/30 23:03:53 INFO impl.YarnClientImpl: Submitted application application_1446245362115_0001\n",
      "15/10/30 23:03:53 INFO mapreduce.Job: The url to track the job: http://radical-9.radical-cybertools.org:8088/proxy/application_1446245362115_0001/\n",
      "15/10/30 23:03:53 INFO mapreduce.Job: Running job: job_1446245362115_0001\n",
      "15/10/30 23:04:01 INFO mapreduce.Job: Job job_1446245362115_0001 running in uber mode : false\n",
      "15/10/30 23:04:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/10/30 23:04:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/10/30 23:04:06 INFO mapreduce.Job: Job job_1446245362115_0001 completed successfully\n",
      "15/10/30 23:04:06 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=251708\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=164\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5955\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=5955\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5955\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12195840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tInput split bytes=164\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tCPU time spent (ms)=3030\n",
      "\t\tPhysical memory (bytes) snapshot=499560448\n",
      "\t\tVirtual memory (bytes) snapshot=7597682688\n",
      "\t\tTotal committed heap usage (bytes)=552075264\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=214574985129000\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar $HADOOP_EXAMPLES teragen 100000 teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use \"yarn jar\" to launch YARN applications.\n",
      "15/10/30 23:05:14 INFO terasort.TeraSort: starting\n",
      "15/10/30 23:05:15 INFO input.FileInputFormat: Total input paths to process : 2\n",
      "Spent 97ms computing base-splits.\n",
      "Spent 2ms computing TeraScheduler splits.\n",
      "Computing input splits took 100ms\n",
      "Sampling 2 splits of 2\n",
      "Making 1 from 100000 sampled records\n",
      "Computing parititions took 278ms\n",
      "Spent 380ms computing partitions.\n",
      "15/10/30 23:05:15 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/30 23:05:15 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "15/10/30 23:05:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/10/30 23:05:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446245362115_0002\n",
      "15/10/30 23:05:16 INFO impl.YarnClientImpl: Submitted application application_1446245362115_0002\n",
      "15/10/30 23:05:16 INFO mapreduce.Job: The url to track the job: http://radical-9.radical-cybertools.org:8088/proxy/application_1446245362115_0002/\n",
      "15/10/30 23:05:16 INFO mapreduce.Job: Running job: job_1446245362115_0002\n",
      "15/10/30 23:05:21 INFO mapreduce.Job: Job job_1446245362115_0002 running in uber mode : false\n",
      "15/10/30 23:05:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/10/30 23:05:26 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/10/30 23:05:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/10/30 23:05:33 INFO mapreduce.Job: Job job_1446245362115_0002 completed successfully\n",
      "15/10/30 23:05:33 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10400006\n",
      "\t\tFILE: Number of bytes written=21181698\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10000242\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5337\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9846\n",
      "\t\tTotal time spent by all map tasks (ms)=5337\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4923\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5337\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4923\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10930176\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=20164608\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tMap output bytes=10200000\n",
      "\t\tMap output materialized bytes=10400012\n",
      "\t\tInput split bytes=242\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100000\n",
      "\t\tReduce shuffle bytes=10400012\n",
      "\t\tReduce input records=100000\n",
      "\t\tReduce output records=100000\n",
      "\t\tSpilled Records=200000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=607\n",
      "\t\tCPU time spent (ms)=17290\n",
      "\t\tPhysical memory (bytes) snapshot=3355975680\n",
      "\t\tVirtual memory (bytes) snapshot=13232820224\n",
      "\t\tTotal committed heap usage (bytes)=3806330880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10000000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n",
      "15/10/30 23:05:33 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar $HADOOP_EXAMPLES terasort teragen teraout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Count\n",
    "\n",
    "Count the words contained in the log file located at `/data/nasa/NASA_access_log_Jul95`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/30 23:51:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\r\n",
      "Moved: 'hdfs://radical-10:8020/user/radical/wordcount-out' to trash at: hdfs://radical-10:8020/user/radical/.Trash/Current\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\r\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\r\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\r\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\r\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\r\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\r\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\r\n",
      "text: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text /data/nasa/NASA_access_log_Jul95 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/30 23:51:20 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/30 23:51:20 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "15/10/30 23:51:20 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "15/10/30 23:51:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/10/30 23:51:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446245362115_0004\n",
      "15/10/30 23:51:21 INFO impl.YarnClientImpl: Submitted application application_1446245362115_0004\n",
      "15/10/30 23:51:21 INFO mapreduce.Job: The url to track the job: http://radical-9.radical-cybertools.org:8088/proxy/application_1446245362115_0004/\n",
      "15/10/30 23:51:21 INFO mapreduce.Job: Running job: job_1446245362115_0004\n",
      "15/10/30 23:51:26 INFO mapreduce.Job: Job job_1446245362115_0004 running in uber mode : false\n",
      "15/10/30 23:51:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/10/30 23:51:38 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "15/10/30 23:51:41 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "15/10/30 23:51:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/10/30 23:51:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/10/30 23:51:49 INFO mapreduce.Job: Job job_1446245362115_0004 completed successfully\n",
      "15/10/30 23:51:49 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=34517909\n",
      "\t\tFILE: Number of bytes written=69414384\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205373678\n",
      "\t\tHDFS: Number of bytes written=29131159\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=31668\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10616\n",
      "\t\tTotal time spent by all map tasks (ms)=31668\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5308\n",
      "\t\tTotal vcore-seconds taken by all map tasks=31668\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5308\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=64856064\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=21741568\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=18915314\n",
      "\t\tMap output bytes=280900888\n",
      "\t\tMap output materialized bytes=34517915\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=18915314\n",
      "\t\tCombine output records=1237021\n",
      "\t\tReduce input groups=1216956\n",
      "\t\tReduce shuffle bytes=34517915\n",
      "\t\tReduce input records=1237021\n",
      "\t\tReduce output records=1216956\n",
      "\t\tSpilled Records=2474042\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=916\n",
      "\t\tCPU time spent (ms)=48910\n",
      "\t\tPhysical memory (bytes) snapshot=3511361536\n",
      "\t\tVirtual memory (bytes) snapshot=13279002624\n",
      "\t\tTotal committed heap usage (bytes)=3949985792\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205373440\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29131159\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_EXAMPLES wordcount /data/nasa/ wordcount-out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Log Parsing\n",
    "\n",
    "Use the commands `head`, `cat`, `uniq`, `wc`, `sort`, `find`, `xargs`, `awk` to evaluate the NASA log file:\n",
    "\n",
    "Which page was called the most?\n",
    "What was the most frequent return code?\n",
    "How many errors occurred? What is the percentage of errors?\n",
    "Implement a Python version of this Unix Shell script using this script as template!\n",
    "\n",
    "Run the Python script inside an Hadoop Streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\r\n",
      "#\r\n",
      "# Licensed to Cloudera, Inc. under one\r\n",
      "# or more contributor license agreements.  See the NOTICE file\r\n",
      "# distributed with this work for additional information\r\n",
      "# regarding copyright ownership.  Cloudera, Inc. licenses this file\r\n",
      "# to you under the Apache License, Version 2.0 (the\r\n",
      "# \"License\"); you may not use this file except in compliance\r\n",
      "# with the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "#\r\n",
      "# Template for python Hadoop streaming.  Fill in the map() and reduce()\r\n",
      "# functions, which should call emit(), as appropriate.\r\n",
      "#\r\n",
      "# Test your script with\r\n",
      "#  cat input | python map_reduce.py map | sort | python wordcount.py reduce\r\n",
      "\r\n",
      "import sys\r\n",
      "import re\r\n",
      "try:\r\n",
      "    import simplejson as json\r\n",
      "except ImportError:\r\n",
      "    import json\r\n",
      "\r\n",
      "import __builtin__\r\n",
      "\r\n",
      "def map(line):\r\n",
      "    try:\r\n",
      "        words = line.split()\r\n",
      "        http_response_code = words[-2]\r\n",
      "        emit(http_response_code, str(1))\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "    \r\n",
      "def reduce(key, values):\r\n",
      "    emit(key, str(sum(__builtin__.map(int,values))))\r\n",
      "\r\n",
      "# Common library code follows:\r\n",
      "\r\n",
      "def emit(key, value):\r\n",
      "    \"\"\"\r\n",
      "    Emits a key->value pair.  Key and value should be strings.\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        print \"\\t\".join( (key, value) )\r\n",
      "    except:\r\n",
      "        pass\r\n",
      "\r\n",
      "def run_map():\r\n",
      "    \"\"\"Calls map() for each input value.\"\"\"\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        map(line)\r\n",
      "\r\n",
      "def run_reduce():\r\n",
      "    \"\"\"Gathers reduce() data in memory, and calls reduce().\"\"\"\r\n",
      "    prev_key = None\r\n",
      "    values = []\r\n",
      "    for line in sys.stdin:\r\n",
      "        line = line.rstrip()\r\n",
      "        key, value = re.split(\"\\t\", line, 1)\r\n",
      "        if prev_key == key:\r\n",
      "            values.append(value)\r\n",
      "        else:\r\n",
      "            if prev_key is not None:\r\n",
      "                reduce(prev_key, values)\r\n",
      "            prev_key = key\r\n",
      "            values = [ value ]\r\n",
      "\r\n",
      "    if prev_key is not None:\r\n",
      "        reduce(prev_key, values)\r\n",
      "\r\n",
      "def main():\r\n",
      "    \"\"\"Runs map or reduce code, per arguments.\"\"\"\r\n",
      "    if len(sys.argv) != 2 or sys.argv[1] not in (\"map\", \"reduce\"):\r\n",
      "        print \"Usage: %s <map|reduce>\" % sys.argv[0]\r\n",
      "        sys.exit(1)\r\n",
      "    if sys.argv[1] == \"map\":\r\n",
      "        run_map()\r\n",
      "    elif sys.argv[1] == \"reduce\":\r\n",
      "        run_reduce()\r\n",
      "    else:\r\n",
      "        assert False\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "  main()\r\n"
     ]
    }
   ],
   "source": [
    "!cat mapreduce_streaming.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/31 00:00:57 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapreduce_streaming.py] [/usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming-2.7.1.2.3.2.0-2950.jar] /var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir/streamjob8755056418131512842.jar tmpDir=null\n",
      "15/10/31 00:00:58 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/31 00:00:58 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "15/10/31 00:00:58 INFO impl.TimelineClientImpl: Timeline service address: http://radical-9:8188/ws/v1/timeline/\n",
      "15/10/31 00:00:58 INFO client.RMProxy: Connecting to ResourceManager at radical-9/10.20.108.87:8050\n",
      "15/10/31 00:00:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.86:50010\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.93:50010\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.90:50010\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.82:50010\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.88:50010\n",
      "15/10/31 00:00:59 INFO net.NetworkTopology: Adding a new node: /default-rack/10.20.108.87:50010\n",
      "15/10/31 00:00:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "15/10/31 00:00:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446245362115_0005\n",
      "15/10/31 00:01:14 INFO impl.YarnClientImpl: Submitted application application_1446245362115_0005\n",
      "15/10/31 00:01:14 INFO mapreduce.Job: The url to track the job: http://radical-9.radical-cybertools.org:8088/proxy/application_1446245362115_0005/\n",
      "15/10/31 00:01:14 INFO mapreduce.Job: Running job: job_1446245362115_0005\n",
      "15/10/31 00:02:41 INFO mapreduce.Job: Job job_1446245362115_0005 running in uber mode : false\n",
      "15/10/31 00:02:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/10/31 00:02:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/10/31 00:03:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/10/31 00:04:04 INFO mapreduce.Job: Job job_1446245362115_0005 completed successfully\n",
      "15/10/31 00:04:05 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15133718\n",
      "\t\tFILE: Number of bytes written=30655053\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205250772\n",
      "\t\tHDFS: Number of bytes written=70\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9157\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17982\n",
      "\t\tTotal time spent by all map tasks (ms)=9157\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8991\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9157\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8991\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18753536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=36827136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1891715\n",
      "\t\tMap output records=1891714\n",
      "\t\tMap output bytes=11350284\n",
      "\t\tMap output materialized bytes=15133724\n",
      "\t\tInput split bytes=212\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=15133724\n",
      "\t\tReduce input records=1891714\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=3783428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=200\n",
      "\t\tCPU time spent (ms)=11600\n",
      "\t\tPhysical memory (bytes) snapshot=1613701120\n",
      "\t\tVirtual memory (bytes) snapshot=13214408704\n",
      "\t\tTotal committed heap usage (bytes)=2368733184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205250560\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70\n",
      "15/10/31 00:04:05 INFO streaming.StreamJob: Output directory: logs-parsed\n"
     ]
    }
   ],
   "source": [
    "!yarn jar $HADOOP_STREAMING -input /data/nasa -output logs-parsed \\\n",
    "                            -file mapreduce_streaming.py \\\n",
    "                            -mapper \"python mapreduce_streaming.py map\" \\\n",
    "                            -reducer \"python mapreduce_streaming.py reduce\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\t1701534\r\n",
      "302\t46573\r\n",
      "304\t132627\r\n",
      "400\t5\r\n",
      "403\t54\r\n",
      "404\t10845\r\n",
      "500\t62\r\n",
      "501\t14\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text logs-parsed/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
