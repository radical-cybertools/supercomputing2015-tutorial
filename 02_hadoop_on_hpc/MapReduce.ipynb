{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HADOOP_EXAMPLES=\"/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar\"\n",
    "HADOOP_STREAMING=\"/usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
      "Instead use the hdfs command for it.\n",
      "\n",
      "15/10/25 15:43:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Configured Capacity: 999334871040 (930.70 GB)\n",
      "Present Capacity: 203635781632 (189.65 GB)\n",
      "DFS Remaining: 203635777536 (189.65 GB)\n",
      "DFS Used: 4096 (4 KB)\n",
      "DFS Used%: 0.00%\n",
      "Under replicated blocks: 0\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:50010 (localhost)\n",
      "Hostname: 172.19.249.236\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 999334871040 (930.70 GB)\n",
      "DFS Used: 4096 (4 KB)\n",
      "Non DFS Used: 795699089408 (741.05 GB)\n",
      "DFS Remaining: 203635777536 (189.65 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 20.38%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Sun Oct 25 15:43:32 EDT 2015\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/25 15:56:57 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "15/10/25 15:56:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/10/25 15:56:59 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:00 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:01 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:02 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:03 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:04 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:05 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:06 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:07 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "15/10/25 15:57:08 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/25 15:49:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/10/25 15:49:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/10/25 15:49:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/10/25 15:49:11 INFO terasort.TeraSort: Generating 100000 using 1\n",
      "15/10/25 15:49:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "15/10/25 15:49:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local598867021_0001\n",
      "15/10/25 15:49:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/10/25 15:49:11 INFO mapreduce.Job: Running job: job_local598867021_0001\n",
      "15/10/25 15:49:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/10/25 15:49:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/10/25 15:49:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "15/10/25 15:49:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/10/25 15:49:11 INFO mapred.LocalJobRunner: Starting task: attempt_local598867021_0001_m_000000_0\n",
      "15/10/25 15:49:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "15/10/25 15:49:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/10/25 15:49:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/10/25 15:49:11 INFO mapred.MapTask: Processing split: org.apache.hadoop.examples.terasort.TeraGen$RangeInputFormat$RangeInputSplit@638e20d3\n",
      "15/10/25 15:49:11 INFO mapred.LocalJobRunner: \n",
      "15/10/25 15:49:12 INFO mapred.Task: Task:attempt_local598867021_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/10/25 15:49:12 INFO mapred.LocalJobRunner: \n",
      "15/10/25 15:49:12 INFO mapred.Task: Task attempt_local598867021_0001_m_000000_0 is allowed to commit now\n",
      "15/10/25 15:49:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local598867021_0001_m_000000_0' to hdfs://localhost/user/luckow/teragen/_temporary/0/task_local598867021_0001_m_000000\n",
      "15/10/25 15:49:12 INFO mapred.LocalJobRunner: map\n",
      "15/10/25 15:49:12 INFO mapred.Task: Task 'attempt_local598867021_0001_m_000000_0' done.\n",
      "15/10/25 15:49:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local598867021_0001_m_000000_0\n",
      "15/10/25 15:49:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/10/25 15:49:12 INFO mapreduce.Job: Job job_local598867021_0001 running in uber mode : false\n",
      "15/10/25 15:49:12 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/10/25 15:49:12 INFO mapreduce.Job: Job job_local598867021_0001 completed successfully\n",
      "15/10/25 15:49:12 INFO mapreduce.Job: Counters: 21\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=273563\n",
      "\t\tFILE: Number of bytes written=550213\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=0\n",
      "\t\tHDFS: Number of bytes written=10000000\n",
      "\t\tHDFS: Number of read operations=4\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100000\n",
      "\t\tMap output records=100000\n",
      "\t\tInput split bytes=82\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=48\n",
      "\t\tTotal committed heap usage (bytes)=77594624\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=214574985129000\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10000000\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar $HADOOP_EXAMPLES teragen 100000 teragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort teragen teraout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
